{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining Final Project - NBA Game Winning Forecasting\n",
    "## Learning Rate Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function - featureEng()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @param X: pandas.DataFrame\n",
    "# @param featureSel: int\n",
    "# @return X: pandas.DataFrame\n",
    "def featureEng(X, featureSel=None):\n",
    "    # Feature Engineering\n",
    "    if not featureSel or featureSel == 0:\n",
    "        return X\n",
    "    if featureSel == 1:\n",
    "        X['PTS_DIFF'] = X['PTS_A'] - X['PTS_B']\n",
    "    elif featureSel == 2:\n",
    "        attriToDrop = ['PTS_A', 'PTS_B']\n",
    "        X = X.drop(columns=attriToDrop)\n",
    "    elif featureSel == 3:\n",
    "        X['PTS_DIFF'] = X['PTS_A'] - X['PTS_B']\n",
    "        attriToDrop = ['PTS_A', 'PTS_B']\n",
    "        X = X.drop(columns=attriToDrop)\n",
    "    elif featureSel == 4:\n",
    "        attriToDrop = [\n",
    "            'FGM_A', 'FGA_A', '3PM_A', '3PA_A', 'FTM_A', 'FTA_A', 'OREB_A', 'DREB_A', 'PF_A', \n",
    "            'FGM_B', 'FGA_B', '3PM_B', '3PA_B', 'FTM_B', 'FTA_B', 'OREB_B', 'DREB_B', 'PF_B'\n",
    "        ]\n",
    "        X['PTS_DIFF'] = X['PTS_A'] - X['PTS_B']\n",
    "        X['STL+BLK_A'] = X['STL_A'] + X['BLK_A']\n",
    "        X['STL+BLK_B'] = X['STL_B'] + X['BLK_B']\n",
    "        attriToDrop += ['PTS_A', 'PTS_B', 'STL_A', 'STL_B', 'BLK_A', 'BLK_B']\n",
    "        X = X.drop(columns=attriToDrop)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function - featureExtraction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @param dfFile: pandas.DataFrame ('nba_preprocessed.csv')\n",
    "# @param dateStart, dateEnd: str in the format of 'YYYY-MM-DD'\n",
    "# @param period: int\n",
    "# @param featureSel: int\n",
    "# @return X, Y: pandas.DataFrame\n",
    "# featureExtraction() outputs X, Y for model training.\n",
    "def featureExtraction(dfFile, dateStart='1000-01-01', dateEnd='2999-12-31', period=5, featureSel=None):\n",
    "    df = pd.read_csv(dfFile)\n",
    "    \n",
    "    # Date selection\n",
    "    df = df.loc[(df.Date_A >= dateStart) & (df.Date_A <= dateEnd), :].reset_index(drop=True)\n",
    "    \n",
    "    # Get label Y\n",
    "    Y = df[['W/L_A']]\n",
    "    Y = Y.rename(columns={'W/L_A': 'Label'})\n",
    "    \n",
    "    # Get averaged attributes X\n",
    "    for idx, row in df.iterrows():\n",
    "        df_sel = df.loc[df.Date_A <= row['Date_A'], :].reset_index(drop=True)\n",
    "        \n",
    "        # Process of Team_A\n",
    "        gamePlayed_A = df_sel.loc[df_sel.Team_A == row['Team_A'], :]\n",
    "        if len(gamePlayed_A) == 1:\n",
    "            X_A = gamePlayed_A.loc[(gamePlayed_A.Team_A == row['Team_A']), :].sort_values(by=['Date_A'], ascending=False).iloc[0:1, 0:24].reset_index(drop=True)\n",
    "        elif len(gamePlayed_A) < period:\n",
    "            X_A = gamePlayed_A.loc[(gamePlayed_A.Team_A == row['Team_A']), :].sort_values(by=['Date_A'], ascending=False).iloc[1:len(gamePlayed_A), 0:24].reset_index(drop=True)\n",
    "        else:\n",
    "            X_A = gamePlayed_A.loc[(gamePlayed_A.Team_A == row['Team_A']), :].sort_values(by=['Date_A'], ascending=False).iloc[1:period+1, 0:24].reset_index(drop=True)\n",
    "        \n",
    "        # Process of Team_B\n",
    "        gamePlayed_B = df_sel.loc[df_sel.Team_A == row['Team_B'], :]\n",
    "        if len(gamePlayed_B) == 1:\n",
    "            X_B = gamePlayed_B.loc[(gamePlayed_B.Team_A == row['Team_B']), :].sort_values(by=['Date_A'], ascending=False).iloc[0:1, 0:24].reset_index(drop=True)\n",
    "        elif len(gamePlayed_B) < period:\n",
    "            X_B = gamePlayed_B.loc[(gamePlayed_B.Team_A == row['Team_B']), :].sort_values(by=['Date_A'], ascending=False).iloc[1:len(gamePlayed_B), 0:24].reset_index(drop=True)\n",
    "        else:\n",
    "            X_B = gamePlayed_B.loc[(gamePlayed_B.Team_A == row['Team_B']), :].sort_values(by=['Date_A'], ascending=False).iloc[1:period+1, 0:24].reset_index(drop=True)\n",
    "        \n",
    "        # Drop unnecessary attributes\n",
    "        colToDrop = ['Home/Away_A'] + ['Team_A', 'Date_A', 'W/L_A', 'Score_A', 'Opponent_A']\n",
    "        X_A = X_A.drop(columns=colToDrop)\n",
    "        X_B = X_B.drop(columns=colToDrop)\n",
    "        \n",
    "        # Rename X_B's columns\n",
    "        X_B = X_B.rename(columns=lambda x: x[0:-2] + '_B')\n",
    "        \n",
    "        # Get X_single = [Home/Away_A + X_A + X_B]\n",
    "        X_single = pd.DataFrame(data=pd.concat([X_A.mean(), X_B.mean()])).transpose()\n",
    "        X_single = pd.concat([pd.DataFrame(data={'Home/Away_A': [row['Home/Away_A']]}), X_single], axis=1)\n",
    "        \n",
    "        # Concatenation dataFrames by row\n",
    "        if idx == 0:\n",
    "            X = X_single\n",
    "        else:\n",
    "            X = pd.concat([X, X_single], ignore_index=True)\n",
    "        \n",
    "    # Feature Engineering\n",
    "    X = featureEng(X, featureSel)\n",
    "        \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function - attriGen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @param dfFile: pandas.DataFrame (from 'nba_preprocessed.csv')\n",
    "# @param date: str in the format of 'YYYY-MM-DD'\n",
    "# @param period: int (Number of previous games to be considered)\n",
    "# @param Team_A, Team_B: str\n",
    "# @param homeAway: int (None for played game prediction)\n",
    "# @param featureSel: int\n",
    "# @return X: pandas.DataFrame\n",
    "def attriGen(df, date, period, Team_A, Team_B, homeAway=None, featureSel=None):\n",
    "    # True Home/Away at the game day\n",
    "    if homeAway is None:\n",
    "        df_gameDay = df.loc[(df.Date_A == date) & (df.Team_A == Team_A) & (df.Team_B == Team_B), :].reset_index(drop=True)\n",
    "        homeAway = int(df_gameDay['Home/Away_A'])\n",
    "    \n",
    "    # Date selections\n",
    "    df = df.loc[df.Date_A < date, :].reset_index(drop=True)\n",
    "    X_A = df.loc[(df.Team_A == Team_A), :].sort_values(by=['Date_A'], ascending=False).iloc[0:period, 0:24].reset_index(drop=True)\n",
    "    X_B = df.loc[(df.Team_A == Team_B), :].sort_values(by=['Date_A'], ascending=False).iloc[0:period, 0:24].reset_index(drop=True)\n",
    "    \n",
    "    # Drop unnecessary attributes\n",
    "    colToDrop = ['Home/Away_A'] + ['Team_A', 'Date_A', 'W/L_A', 'Score_A', 'Opponent_A']\n",
    "    X_A = X_A.drop(columns=colToDrop)\n",
    "    X_B = X_B.drop(columns=colToDrop)\n",
    "    \n",
    "    # Rename X_away's columns\n",
    "    X_B = X_B.rename(columns=lambda x: x[0:-2] + '_B')\n",
    "    \n",
    "    # Get X = [Home/Away_A + X_A + X_B]\n",
    "    X = pd.DataFrame(data=pd.concat([X_A.mean(), X_B.mean()])).transpose()\n",
    "    X = pd.concat([pd.DataFrame(data={'Home/Away_A': [homeAway]}), X], axis=1)\n",
    "    \n",
    "    # Feature Engineering\n",
    "    X = featureEng(X, featureSel)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function - groundTruthGen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @param dfFile: pandas.DataFrame (from 'nba_preprocessed.csv')\n",
    "# @param date: str in the format of 'YYYY-MM-DD'\n",
    "# @param Team_A, Team_B: str\n",
    "# @param featureSel: int\n",
    "# @return X_groundTruth, Y_groundTruth: pandas.DataFrame\n",
    "def groundTruthGen(df, date, Team_A, Team_B, featureSel=None):\n",
    "    # Date selections\n",
    "    df = df.loc[(df.Date_A == date) & (df.Team_A == Team_A) & (df.Team_B == Team_B), :].reset_index(drop=True)\n",
    "\n",
    "    # Get label Y\n",
    "    Y_groundTruth = df[['W/L_A']]\n",
    "    Y_groundTruth = Y_groundTruth.rename(columns={'W/L_A': 'Label'})\n",
    "    \n",
    "    # Drop unnecessary attributes\n",
    "    colToDrop = [\n",
    "        'Team_A', 'Date_A', 'W/L_A', 'Score_A', 'Opponent_A', \n",
    "        'Team_B', 'Date_B', 'W/L_B', 'Home/Away_B', 'Score_B', 'Opponent_B'\n",
    "    ]\n",
    "    X_groundTruth = df.drop(columns=colToDrop)\n",
    "    \n",
    "    # Feature Engineering\n",
    "    X_groundTruth = featureEng(X_groundTruth, featureSel)\n",
    "    \n",
    "    return X_groundTruth, Y_groundTruth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function - gameAttriGen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @param dfFile: pandas.DataFrame ('nba_preprocessed.csv')\n",
    "# @param dateStart, dateEnd: str in the format of 'YYYY-MM-DD'\n",
    "# @param period: int\n",
    "# @param Team_A, Team_B: str (If both are None, predict all games within the date range)\n",
    "# @param featureSel: int\n",
    "# @return X, Y: pandas.DataFrame\n",
    "# gameAttriGen() outputs X_attri, Y_truth for game prediction.\n",
    "def gameAttriGen(dfFile, dateStart, dateEnd, period=5, Team_A=None, Team_B=None, featureSel=None):\n",
    "    df = pd.read_csv(dfFile)\n",
    "    \n",
    "    # Date selections\n",
    "    df_sel = df.loc[(df.Date_A >= dateStart) & (df.Date_A <= dateEnd), :].reset_index(drop=True)\n",
    "    \n",
    "    # Generate df_sel which includes [date, Team_A, Team_B] columns\n",
    "    if Team_A and Team_B:\n",
    "        df_sel = df_sel.loc[(df_sel.Team_A == Team_A) & (df_sel.Opponent_A == Team_B), :].reset_index(drop=True)[['Date_A', 'Team_A', 'Opponent_A']]\n",
    "    elif Team_A and not Team_B:\n",
    "        df_sel = df_sel.loc[df_sel.Team_A == Team_A, :].reset_index(drop=True)[['Date_A', 'Team_A', 'Opponent_A']]\n",
    "    elif not Team_A and Team_B:\n",
    "        df_sel = df_sel.loc[df_sel.Opponent_A == Team_B, :].reset_index(drop=True)[['Date_A', 'Team_A', 'Opponent_A']]\n",
    "    elif not Team_A and not Team_B:\n",
    "        df_sel = df_sel[['Date_A', 'Team_A', 'Opponent_A']]\n",
    "        # Delete duplicates: (Team_A vs Team_B) is the same as (Team_B vs Team_A). Remove one to avoid double count.\n",
    "        df_new = pd.DataFrame(columns=['Date_A', 'Team_A', 'Opponent_A'])\n",
    "        LUT = {}\n",
    "        for date, x, y in zip(df_sel['Date_A'], df_sel['Team_A'], df_sel['Opponent_A']):\n",
    "            if (date + x + y) in LUT:\n",
    "                df_new = pd.concat([df_new, pd.DataFrame(columns=['Date_A', 'Team_A', 'Opponent_A'], data=[[date, x, y]])], ignore_index=True)\n",
    "            else:\n",
    "                LUT[date + x + y] = 1\n",
    "                LUT[date + y + x] = 1\n",
    "        df_sel = df_new\n",
    "    \n",
    "    # W/L prediction\n",
    "    X_attri = Y_truth = None\n",
    "    for date, Team_A, Team_B in zip(df_sel['Date_A'], df_sel['Team_A'], df_sel['Opponent_A']):\n",
    "        X_toBePredicted = attriGen(df, date, period, Team_A, Team_B, None, featureSel)\n",
    "        X_groundTruth, Y_groundTruth = groundTruthGen(df, date, Team_A, Team_B, featureSel)\n",
    "        if X_attri is None and Y_truth is None:\n",
    "            X_attri = X_toBePredicted\n",
    "            Y_truth = Y_groundTruth\n",
    "        else:\n",
    "            X_attri = pd.concat([X_attri, X_toBePredicted], ignore_index=True)\n",
    "            Y_truth = pd.concat([Y_truth, Y_groundTruth], ignore_index=True)\n",
    "        \n",
    "    return X_attri, Y_truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function - gamePrediction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @param dfFile: pandas.DataFrame ('nba_preprocessed.csv')\n",
    "# @param modelsLUT: dict in the format of {'modelName': model}\n",
    "# @param dateStart, dateEnd: str in the format of 'YYYY-MM-DD'\n",
    "# @param period: int (Number of previous games to be considered)\n",
    "# @param Team_A, Team_B: str (If both are None, predict all games within the date range)\n",
    "# @param featureSel: int\n",
    "# @return None\n",
    "# gamePrediction() prints the predicted game W/L results.\n",
    "def gamePrediction(dfFile, modelsLUT, dateStart, dateEnd, period=5, Team_A=None, Team_B=None, featureSel=None):\n",
    "    X_attri, Y_truth = gameAttriGen(dfFile, dateStart, dateEnd, period, Team_A, Team_B, featureSel)\n",
    "    \n",
    "    resultLUT, accuLUT = {}, {}\n",
    "    for model in modelsLUT:\n",
    "        resultLUT[model] = modelsLUT[model].predict(X_attri)\n",
    "        accuLUT[model] = accuracy_score(Y_truth, modelsLUT[model].predict(X_attri))\n",
    "    \n",
    "    print('---------- Prediction Accuracy ----------')\n",
    "    print('featureSel =', featureSel)\n",
    "    for x in accuLUT:\n",
    "        print(x, '=', accuLUT[x]*100, '%')\n",
    "    print('------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-Year Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Training Data Volume ----------\n",
      "# of data = 2460\n",
      "------------------------------------\n",
      "---------- Prediction Accuracy ----------\n",
      "featureSel = 3\n",
      "logiRegr = 69.62025316455697 %\n",
      "logiRegrCVGS = 69.62025316455697 %\n",
      "supVecMachine = 70.88607594936708 %\n",
      "supVecMachineCVGS = 70.88607594936708 %\n",
      "xgbc = 74.68354430379746 %\n",
      "xgbcCVGS = 74.68354430379746 %\n",
      "naiveBayes = 60.75949367088608 %\n",
      "randomForest = 50.63291139240506 %\n",
      "randomForestCVGS = 68.35443037974683 %\n",
      "gbdt = 72.15189873417721 %\n",
      "gbdtCVGS = 65.82278481012658 %\n",
      "lgbm = 68.35443037974683 %\n",
      "lgbmCVGS = 58.22784810126582 %\n",
      "adaBoost = 68.35443037974683 %\n",
      "adaBoostCVGS = 73.41772151898735 %\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "# Feature Extraction\n",
    "dfFile = 'nba_preprocessed.csv'\n",
    "dateStart = '2017-08-01'\n",
    "dateEnd = '2018-04-13'\n",
    "period = 5\n",
    "featureSel = 3\n",
    "X, Y = featureExtraction(dfFile, dateStart, dateEnd, period, featureSel)\n",
    "\n",
    "# Model Training\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "logiRegr = LogisticRegression()\n",
    "logiRegr.fit(X, Y)\n",
    "logiRegrCVGS = LogisticRegression(C=10, max_iter=300)\n",
    "logiRegrCVGS.fit(X, Y)\n",
    "\n",
    "supVecMachine = SVC(kernel='linear', probability=True)\n",
    "supVecMachine.fit(X, Y)\n",
    "supVecMachineCVGS = SVC(C=1, kernel='linear', probability=True)\n",
    "supVecMachineCVGS.fit(X, Y)\n",
    "\n",
    "xgbc = XGBClassifier()\n",
    "xgbc.fit(X, Y)\n",
    "xgbcCVGS = XGBClassifier(max_depth=3, learning_rate=0.1, n_estimators=100, min_child_weight=3, gamma=0.3)\n",
    "xgbcCVGS.fit(X, Y)\n",
    "\n",
    "naiveBayes = GaussianNB()\n",
    "naiveBayes.fit(X, Y)\n",
    "\n",
    "randomForest = RandomForestClassifier()\n",
    "randomForest.fit(X, Y)\n",
    "randomForestCVGS = RandomForestClassifier(n_estimators=1000, criterion='entropy', bootstrap=True, max_depth=10, max_features='sqrt')\n",
    "randomForestCVGS.fit(X, Y)\n",
    "\n",
    "gbdt = GradientBoostingClassifier()\n",
    "gbdt.fit(X,Y)\n",
    "gbdtCVGS = GradientBoostingClassifier(loss='exponential', n_estimators=800, learning_rate=0.1, max_depth=10, subsample=0.5, max_features='auto')\n",
    "gbdtCVGS.fit(X,Y)\n",
    "\n",
    "lgbm = LGBMClassifier()\n",
    "lgbm.fit(X, Y)\n",
    "lgbmCVGS = LGBMClassifier(learning_rate=0.1, n_estimators=1000, max_depth=-1, subsample=0.5)\n",
    "lgbmCVGS.fit(X, Y)\n",
    "\n",
    "adaBoost = AdaBoostClassifier()\n",
    "adaBoost.fit(X, Y)\n",
    "adaBoostCVGS = AdaBoostClassifier(learning_rate=0.2, n_estimators=50)\n",
    "adaBoostCVGS.fit(X, Y)\n",
    "\n",
    "# Prediction\n",
    "dateStart = '2018-04-14'\n",
    "dateEnd = '2018-06-01'\n",
    "Team_A = None\n",
    "Team_B = None\n",
    "modelsLUT = {\n",
    "    'logiRegr': logiRegr,\n",
    "    'logiRegrCVGS': logiRegrCVGS,\n",
    "    'supVecMachine': supVecMachine,\n",
    "    'supVecMachineCVGS': supVecMachineCVGS,\n",
    "    'xgbc': xgbc,\n",
    "    'xgbcCVGS': xgbcCVGS,\n",
    "    'naiveBayes': naiveBayes,\n",
    "    'randomForest': randomForest,\n",
    "    'randomForestCVGS': randomForestCVGS,\n",
    "    'gbdt': gbdt,\n",
    "    'gbdtCVGS': gbdtCVGS,\n",
    "    'lgbm': lgbm,\n",
    "    'lgbmCVGS': lgbmCVGS,\n",
    "    'adaBoost': adaBoost, \n",
    "    'adaBoostCVGS': adaBoostCVGS\n",
    "}\n",
    "\n",
    "# Training Data Volume\n",
    "print('---------- Training Data Volume ----------')\n",
    "print('# of data =', len(X))\n",
    "print('------------------------------------')\n",
    "\n",
    "# W/L prediction\n",
    "gamePrediction(dfFile, modelsLUT, dateStart, dateEnd, period, Team_A, Team_B, featureSel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-Year Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Training Data Volume ----------\n",
      "# of data = 5078\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Prediction Accuracy ----------\n",
      "featureSel = 3\n",
      "logiRegr = 70.88607594936708 %\n",
      "logiRegrCVGS = 69.62025316455697 %\n",
      "supVecMachine = 72.15189873417721 %\n",
      "supVecMachineCVGS = 72.15189873417721 %\n",
      "xgbc = 70.88607594936708 %\n",
      "xgbcCVGS = 72.15189873417721 %\n",
      "naiveBayes = 59.49367088607595 %\n",
      "randomForest = 55.69620253164557 %\n",
      "randomForestCVGS = 69.62025316455697 %\n",
      "gbdt = 69.62025316455697 %\n",
      "gbdtCVGS = 68.35443037974683 %\n",
      "lgbm = 74.68354430379746 %\n",
      "lgbmCVGS = 62.0253164556962 %\n",
      "adaBoost = 73.41772151898735 %\n",
      "adaBoostCVGS = 68.35443037974683 %\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "# Feature Extraction\n",
    "dfFile = 'nba_preprocessed.csv'\n",
    "dateStart = '2016-08-01'\n",
    "dateEnd = '2018-04-13'\n",
    "period = 5\n",
    "featureSel = 3\n",
    "X, Y = featureExtraction(dfFile, dateStart, dateEnd, period, featureSel)\n",
    "\n",
    "# Model Training\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "logiRegr = LogisticRegression()\n",
    "logiRegr.fit(X, Y)\n",
    "logiRegrCVGS = LogisticRegression(C=1000, max_iter=300)\n",
    "logiRegrCVGS.fit(X, Y)\n",
    "\n",
    "supVecMachine = SVC(kernel='linear', probability=True)\n",
    "supVecMachine.fit(X, Y)\n",
    "supVecMachineCVGS = SVC(C=0.1, kernel='linear', probability=True)\n",
    "supVecMachineCVGS.fit(X, Y)\n",
    "\n",
    "xgbc = XGBClassifier()\n",
    "xgbc.fit(X, Y)\n",
    "xgbcCVGS = XGBClassifier(max_depth=3, learning_rate=0.1, n_estimators=100, min_child_weight=1, gamma=0.4)\n",
    "xgbcCVGS.fit(X, Y)\n",
    "\n",
    "naiveBayes = GaussianNB()\n",
    "naiveBayes.fit(X, Y)\n",
    "\n",
    "randomForest = RandomForestClassifier()\n",
    "randomForest.fit(X, Y)\n",
    "randomForestCVGS = RandomForestClassifier(n_estimators=1000, criterion='entropy', bootstrap=True, max_depth=None, max_features='auto')\n",
    "randomForestCVGS.fit(X, Y)\n",
    "\n",
    "gbdt = GradientBoostingClassifier()\n",
    "gbdt.fit(X,Y)\n",
    "gbdtCVGS = GradientBoostingClassifier(loss='exponential', n_estimators=800, learning_rate=0.1, max_depth=10, subsample=0.5, max_features='sqrt')\n",
    "gbdtCVGS.fit(X,Y)\n",
    "\n",
    "lgbm = LGBMClassifier()\n",
    "lgbm.fit(X, Y)\n",
    "lgbmCVGS = LGBMClassifier(learning_rate=0.1, n_estimators=800, max_depth=10, subsample=0.5)\n",
    "lgbmCVGS.fit(X, Y)\n",
    "\n",
    "adaBoost = AdaBoostClassifier()\n",
    "adaBoost.fit(X, Y)\n",
    "adaBoostCVGS = AdaBoostClassifier(learning_rate=0.3, n_estimators=50)\n",
    "adaBoostCVGS.fit(X, Y)\n",
    "\n",
    "# Prediction\n",
    "dateStart = '2018-04-14'\n",
    "dateEnd = '2018-06-01'\n",
    "Team_A = None\n",
    "Team_B = None\n",
    "modelsLUT = {\n",
    "    'logiRegr': logiRegr,\n",
    "    'logiRegrCVGS': logiRegrCVGS,\n",
    "    'supVecMachine': supVecMachine,\n",
    "    'supVecMachineCVGS': supVecMachineCVGS,\n",
    "    'xgbc': xgbc,\n",
    "    'xgbcCVGS': xgbcCVGS,\n",
    "    'naiveBayes': naiveBayes,\n",
    "    'randomForest': randomForest,\n",
    "    'randomForestCVGS': randomForestCVGS,\n",
    "    'gbdt': gbdt,\n",
    "    'gbdtCVGS': gbdtCVGS,\n",
    "    'lgbm': lgbm,\n",
    "    'lgbmCVGS': lgbmCVGS,\n",
    "    'adaBoost': adaBoost, \n",
    "    'adaBoostCVGS': adaBoostCVGS\n",
    "}\n",
    "\n",
    "# Training Data Volume\n",
    "print('---------- Training Data Volume ----------')\n",
    "print('# of data =', len(X))\n",
    "print('------------------------------------')\n",
    "\n",
    "# W/L prediction\n",
    "gamePrediction(dfFile, modelsLUT, dateStart, dateEnd, period, Team_A, Team_B, featureSel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-Year Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Training Data Volume ----------\n",
      "# of data = 7234\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Prediction Accuracy ----------\n",
      "featureSel = 3\n",
      "logiRegr = 70.88607594936708 %\n",
      "logiRegrCVGS = 69.62025316455697 %\n",
      "supVecMachine = 74.68354430379746 %\n",
      "supVecMachineCVGS = 72.15189873417721 %\n",
      "xgbc = 72.15189873417721 %\n",
      "xgbcCVGS = 74.68354430379746 %\n",
      "naiveBayes = 60.75949367088608 %\n",
      "randomForest = 64.55696202531645 %\n",
      "randomForestCVGS = 70.88607594936708 %\n",
      "gbdt = 73.41772151898735 %\n",
      "gbdtCVGS = 64.55696202531645 %\n",
      "lgbm = 68.35443037974683 %\n",
      "lgbmCVGS = 67.08860759493672 %\n",
      "adaBoost = 70.88607594936708 %\n",
      "adaBoostCVGS = 75.9493670886076 %\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Feature Extraction\n",
    "dfFile = 'nba_preprocessed.csv'\n",
    "dateStart = '2015-08-01'\n",
    "dateEnd = '2018-04-13'\n",
    "period = 5\n",
    "featureSel = 3\n",
    "X, Y = featureExtraction(dfFile, dateStart, dateEnd, period, featureSel)\n",
    "\n",
    "# Model Training\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "logiRegr = LogisticRegression()\n",
    "logiRegr.fit(X, Y)\n",
    "logiRegrCVGS = LogisticRegression(C=100, max_iter=400)\n",
    "logiRegrCVGS.fit(X, Y)\n",
    "\n",
    "supVecMachine = SVC(kernel='linear', probability=True)\n",
    "supVecMachine.fit(X, Y)\n",
    "supVecMachineCVGS = SVC(C=10, kernel='linear', probability=True)\n",
    "supVecMachineCVGS.fit(X, Y)\n",
    "\n",
    "xgbc = XGBClassifier()\n",
    "xgbc.fit(X, Y)\n",
    "xgbcCVGS = XGBClassifier(max_depth=3, learning_rate=0.1, n_estimators=100, min_child_weight=3, gamma=0.2)\n",
    "xgbcCVGS.fit(X, Y)\n",
    "\n",
    "naiveBayes = GaussianNB()\n",
    "naiveBayes.fit(X, Y)\n",
    "\n",
    "randomForest = RandomForestClassifier()\n",
    "randomForest.fit(X, Y)\n",
    "randomForestCVGS = RandomForestClassifier(n_estimators=600, criterion='entropy', bootstrap=True, max_depth=None, max_features='sqrt')\n",
    "randomForestCVGS.fit(X, Y)\n",
    "\n",
    "gbdt = GradientBoostingClassifier()\n",
    "gbdt.fit(X,Y)\n",
    "gbdtCVGS = GradientBoostingClassifier(loss='exponential', n_estimators=600, learning_rate=0.1, max_depth=3, subsample=0.5, max_features='sqrt')\n",
    "gbdtCVGS.fit(X,Y)\n",
    "\n",
    "lgbm = LGBMClassifier()\n",
    "lgbm.fit(X, Y)\n",
    "lgbmCVGS = LGBMClassifier(learning_rate=0.1, n_estimators=800, max_depth=-1, subsample=0.5)\n",
    "lgbmCVGS.fit(X, Y)\n",
    "\n",
    "adaBoost = AdaBoostClassifier()\n",
    "adaBoost.fit(X, Y)\n",
    "adaBoostCVGS = AdaBoostClassifier(learning_rate=0.1, n_estimators=1000)\n",
    "adaBoostCVGS.fit(X, Y)\n",
    "\n",
    "# Prediction\n",
    "dateStart = '2018-04-14'\n",
    "dateEnd = '2018-06-01'\n",
    "Team_A = None\n",
    "Team_B = None\n",
    "modelsLUT = {\n",
    "    'logiRegr': logiRegr,\n",
    "    'logiRegrCVGS': logiRegrCVGS,\n",
    "    'supVecMachine': supVecMachine,\n",
    "    'supVecMachineCVGS': supVecMachineCVGS,\n",
    "    'xgbc': xgbc,\n",
    "    'xgbcCVGS': xgbcCVGS,\n",
    "    'naiveBayes': naiveBayes,\n",
    "    'randomForest': randomForest,\n",
    "    'randomForestCVGS': randomForestCVGS,\n",
    "    'gbdt': gbdt,\n",
    "    'gbdtCVGS': gbdtCVGS,\n",
    "    'lgbm': lgbm,\n",
    "    'lgbmCVGS': lgbmCVGS,\n",
    "    'adaBoost': adaBoost, \n",
    "    'adaBoostCVGS': adaBoostCVGS\n",
    "}\n",
    "\n",
    "# Training Data Volume\n",
    "print('---------- Training Data Volume ----------')\n",
    "print('# of data =', len(X))\n",
    "print('------------------------------------')\n",
    "\n",
    "# W/L prediction\n",
    "gamePrediction(dfFile, modelsLUT, dateStart, dateEnd, period, Team_A, Team_B, featureSel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-Year Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Training Data Volume ----------\n",
      "# of data = 9370\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Prediction Accuracy ----------\n",
      "featureSel = 3\n",
      "logiRegr = 69.62025316455697 %\n",
      "logiRegrCVGS = 69.62025316455697 %\n",
      "supVecMachine = 72.15189873417721 %\n",
      "supVecMachineCVGS = 70.88607594936708 %\n",
      "xgbc = 70.88607594936708 %\n",
      "xgbcCVGS = 72.15189873417721 %\n",
      "naiveBayes = 59.49367088607595 %\n",
      "randomForest = 59.49367088607595 %\n",
      "randomForestCVGS = 72.15189873417721 %\n",
      "gbdt = 70.88607594936708 %\n",
      "gbdtCVGS = 69.62025316455697 %\n",
      "lgbm = 73.41772151898735 %\n",
      "lgbmCVGS = 63.29113924050633 %\n",
      "adaBoost = 72.15189873417721 %\n",
      "adaBoostCVGS = 73.41772151898735 %\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Feature Extraction\n",
    "dfFile = 'nba_preprocessed.csv'\n",
    "dateStart = '2014-08-01'\n",
    "dateEnd = '2018-04-13'\n",
    "period = 5\n",
    "featureSel = 3\n",
    "X, Y = featureExtraction(dfFile, dateStart, dateEnd, period, featureSel)\n",
    "\n",
    "# Model Training\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "logiRegr = LogisticRegression()\n",
    "logiRegr.fit(X, Y)\n",
    "logiRegrCVGS = LogisticRegression(C=100, max_iter=400)\n",
    "logiRegrCVGS.fit(X, Y)\n",
    "\n",
    "supVecMachine = SVC(kernel='linear', probability=True)\n",
    "supVecMachine.fit(X, Y)\n",
    "supVecMachineCVGS = SVC(C=0.01, kernel='linear', probability=True)\n",
    "supVecMachineCVGS.fit(X, Y)\n",
    "\n",
    "xgbc = XGBClassifier()\n",
    "xgbc.fit(X, Y)\n",
    "xgbcCVGS = XGBClassifier(max_depth=5, learning_rate=0.1, n_estimators=100, min_child_weight=1, gamma=0.4)\n",
    "xgbcCVGS.fit(X, Y)\n",
    "\n",
    "naiveBayes = GaussianNB()\n",
    "naiveBayes.fit(X, Y)\n",
    "\n",
    "randomForest = RandomForestClassifier()\n",
    "randomForest.fit(X, Y)\n",
    "randomForestCVGS = RandomForestClassifier(n_estimators=1000, criterion='entropy', bootstrap=True, max_depth=None, max_features='log2')\n",
    "randomForestCVGS.fit(X, Y)\n",
    "\n",
    "gbdt = GradientBoostingClassifier()\n",
    "gbdt.fit(X,Y)\n",
    "gbdtCVGS = GradientBoostingClassifier(loss='exponential', n_estimators=600, learning_rate=0.1, max_depth=3, subsample=0.5, max_features='sqrt')\n",
    "gbdtCVGS.fit(X,Y)\n",
    "\n",
    "lgbm = LGBMClassifier()\n",
    "lgbm.fit(X, Y)\n",
    "lgbmCVGS = LGBMClassifier(learning_rate=0.1, n_estimators=800, max_depth=5, subsample=0.5)\n",
    "lgbmCVGS.fit(X, Y)\n",
    "\n",
    "adaBoost = AdaBoostClassifier()\n",
    "adaBoost.fit(X, Y)\n",
    "adaBoostCVGS = AdaBoostClassifier(learning_rate=0.1, n_estimators=600)\n",
    "adaBoostCVGS.fit(X, Y)\n",
    "\n",
    "# Prediction\n",
    "dateStart = '2018-04-14'\n",
    "dateEnd = '2018-06-01'\n",
    "Team_A = None\n",
    "Team_B = None\n",
    "modelsLUT = {\n",
    "    'logiRegr': logiRegr,\n",
    "    'logiRegrCVGS': logiRegrCVGS,\n",
    "    'supVecMachine': supVecMachine,\n",
    "    'supVecMachineCVGS': supVecMachineCVGS,\n",
    "    'xgbc': xgbc,\n",
    "    'xgbcCVGS': xgbcCVGS,\n",
    "    'naiveBayes': naiveBayes,\n",
    "    'randomForest': randomForest,\n",
    "    'randomForestCVGS': randomForestCVGS,\n",
    "    'gbdt': gbdt,\n",
    "    'gbdtCVGS': gbdtCVGS,\n",
    "    'lgbm': lgbm,\n",
    "    'lgbmCVGS': lgbmCVGS,\n",
    "    'adaBoost': adaBoost, \n",
    "    'adaBoostCVGS': adaBoostCVGS\n",
    "}\n",
    "\n",
    "# Training Data Volume\n",
    "print('---------- Training Data Volume ----------')\n",
    "print('# of data =', len(X))\n",
    "print('------------------------------------')\n",
    "\n",
    "# W/L prediction\n",
    "gamePrediction(dfFile, modelsLUT, dateStart, dateEnd, period, Team_A, Team_B, featureSel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-Year Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Training Data Volume ----------\n",
      "# of data = 11702\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Prediction Accuracy ----------\n",
      "featureSel = 3\n",
      "logiRegr = 70.88607594936708 %\n",
      "logiRegrCVGS = 69.62025316455697 %\n",
      "supVecMachine = 70.88607594936708 %\n",
      "supVecMachineCVGS = 70.88607594936708 %\n",
      "xgbc = 75.9493670886076 %\n",
      "xgbcCVGS = 75.9493670886076 %\n",
      "naiveBayes = 59.49367088607595 %\n",
      "randomForest = 55.69620253164557 %\n",
      "randomForestCVGS = 73.41772151898735 %\n",
      "gbdt = 74.68354430379746 %\n",
      "gbdtCVGS = 70.88607594936708 %\n",
      "lgbm = 69.62025316455697 %\n",
      "lgbmCVGS = 63.29113924050633 %\n",
      "adaBoost = 67.08860759493672 %\n",
      "adaBoostCVGS = 74.68354430379746 %\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Feature Extraction\n",
    "dfFile = 'nba_preprocessed.csv'\n",
    "dateStart = '2013-08-01'\n",
    "dateEnd = '2018-04-13'\n",
    "period = 5\n",
    "featureSel = 3\n",
    "X, Y = featureExtraction(dfFile, dateStart, dateEnd, period, featureSel)\n",
    "\n",
    "# Model Training\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "logiRegr = LogisticRegression()\n",
    "logiRegr.fit(X, Y)\n",
    "logiRegrCVGS = LogisticRegression(C=1000, max_iter=500)\n",
    "logiRegrCVGS.fit(X, Y)\n",
    "\n",
    "supVecMachine = SVC(kernel='linear', probability=True)\n",
    "supVecMachine.fit(X, Y)\n",
    "supVecMachineCVGS = SVC(C=10, kernel='linear', probability=True)\n",
    "supVecMachineCVGS.fit(X, Y)\n",
    "\n",
    "xgbc = XGBClassifier()\n",
    "xgbc.fit(X, Y)\n",
    "xgbcCVGS = XGBClassifier(max_depth=3, learning_rate=0.1, n_estimators=200, min_child_weight=1, gamma=0.1)\n",
    "xgbcCVGS.fit(X, Y)\n",
    "\n",
    "naiveBayes = GaussianNB()\n",
    "naiveBayes.fit(X, Y)\n",
    "\n",
    "randomForest = RandomForestClassifier()\n",
    "randomForest.fit(X, Y)\n",
    "randomForestCVGS = RandomForestClassifier(n_estimators=1000, criterion='entropy', bootstrap=True, max_depth=None, max_features='log2')\n",
    "randomForestCVGS.fit(X, Y)\n",
    "\n",
    "gbdt = GradientBoostingClassifier()\n",
    "gbdt.fit(X,Y)\n",
    "gbdtCVGS = GradientBoostingClassifier(loss='exponential', n_estimators=600, learning_rate=0.1, max_depth=3, subsample=0.5, max_features='sqrt')\n",
    "gbdtCVGS.fit(X,Y)\n",
    "\n",
    "lgbm = LGBMClassifier()\n",
    "lgbm.fit(X, Y)\n",
    "lgbmCVGS = LGBMClassifier(learning_rate=0.1, n_estimators=600, max_depth=5, subsample=0.5)\n",
    "lgbmCVGS.fit(X, Y)\n",
    "\n",
    "adaBoost = AdaBoostClassifier()\n",
    "adaBoost.fit(X, Y)\n",
    "adaBoostCVGS = AdaBoostClassifier(learning_rate=0.1, n_estimators=1000)\n",
    "adaBoostCVGS.fit(X, Y)\n",
    "\n",
    "# Prediction\n",
    "dateStart = '2018-04-14'\n",
    "dateEnd = '2018-06-01'\n",
    "Team_A = None\n",
    "Team_B = None\n",
    "modelsLUT = {\n",
    "    'logiRegr': logiRegr,\n",
    "    'logiRegrCVGS': logiRegrCVGS,\n",
    "    'supVecMachine': supVecMachine,\n",
    "    'supVecMachineCVGS': supVecMachineCVGS,\n",
    "    'xgbc': xgbc,\n",
    "    'xgbcCVGS': xgbcCVGS,\n",
    "    'naiveBayes': naiveBayes,\n",
    "    'randomForest': randomForest,\n",
    "    'randomForestCVGS': randomForestCVGS,\n",
    "    'gbdt': gbdt,\n",
    "    'gbdtCVGS': gbdtCVGS,\n",
    "    'lgbm': lgbm,\n",
    "    'lgbmCVGS': lgbmCVGS,\n",
    "    'adaBoost': adaBoost, \n",
    "    'adaBoostCVGS': adaBoostCVGS\n",
    "}\n",
    "\n",
    "# Training Data Volume\n",
    "print('---------- Training Data Volume ----------')\n",
    "print('# of data =', len(X))\n",
    "print('------------------------------------')\n",
    "\n",
    "# W/L prediction\n",
    "gamePrediction(dfFile, modelsLUT, dateStart, dateEnd, period, Team_A, Team_B, featureSel)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
