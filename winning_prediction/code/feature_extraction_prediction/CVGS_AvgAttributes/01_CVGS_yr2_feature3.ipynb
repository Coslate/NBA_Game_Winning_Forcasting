{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation & Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter Settings\n",
    "dfFile = 'nba_preprocessed.csv'\n",
    "dateStart = '2016-08-01'\n",
    "dateEnd = '2018-04-13'\n",
    "trialName = '00_model_CVGS_yr2_feature3_param2'\n",
    "period = 5\n",
    "featureSel = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import csv\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# CVGS\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Estimators\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @param X: pandas.DataFrame\n",
    "# @param featureSel: int\n",
    "# @return X: pandas.DataFrame\n",
    "def featureEng(X, featureSel=None):\n",
    "    # Feature Engineering\n",
    "    if not featureSel or featureSel == 0:\n",
    "        return X\n",
    "    if featureSel == 1:\n",
    "        X['PTS_DIFF'] = X['PTS_A'] - X['PTS_B']\n",
    "    elif featureSel == 2:\n",
    "        attriToDrop = ['PTS_A', 'PTS_B']\n",
    "        X = X.drop(columns=attriToDrop)\n",
    "    elif featureSel == 3:\n",
    "        X['PTS_DIFF'] = X['PTS_A'] - X['PTS_B']\n",
    "        attriToDrop = ['PTS_A', 'PTS_B']\n",
    "        X = X.drop(columns=attriToDrop)\n",
    "    elif featureSel == 4:\n",
    "        attriToDrop = [\n",
    "            'FGM_A', 'FGA_A', '3PM_A', '3PA_A', 'FTM_A', 'FTA_A', 'OREB_A', 'DREB_A', 'PF_A', \n",
    "            'FGM_B', 'FGA_B', '3PM_B', '3PA_B', 'FTM_B', 'FTA_B', 'OREB_B', 'DREB_B', 'PF_B'\n",
    "        ]\n",
    "        X['PTS_DIFF'] = X['PTS_A'] - X['PTS_B']\n",
    "        X['STL+BLK_A'] = X['STL_A'] + X['BLK_A']\n",
    "        X['STL+BLK_B'] = X['STL_B'] + X['BLK_B']\n",
    "        attriToDrop += ['PTS_A', 'PTS_B', 'STL_A', 'STL_B', 'BLK_A', 'BLK_B']\n",
    "        X = X.drop(columns=attriToDrop)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @param dfFile: pandas.DataFrame ('nba_preprocessed.csv')\n",
    "# @param dateStart, dateEnd: str in the format of 'YYYY-MM-DD'\n",
    "# @param period: int\n",
    "# @param featureSel: int (0, 1, 2, and 3 corresponds to feature0, 1, 2, and 3, respectively)\n",
    "# @return X, Y: pandas.DataFrame\n",
    "# featureExtraction() outputs X, Y for model training.\n",
    "# Game date can be assigned\n",
    "# Attribute to be dropped can be assigned\n",
    "def featureExtraction(dfFile, dateStart='1000-01-01', dateEnd='2999-12-31', period=5, featureSel=None):\n",
    "    df = pd.read_csv(dfFile)\n",
    "    \n",
    "    # Date selection\n",
    "    df = df.loc[(df.Date_A >= dateStart) & (df.Date_A <= dateEnd), :].reset_index(drop=True)\n",
    "    \n",
    "    # Get label Y\n",
    "    Y = df[['W/L_A']]\n",
    "    Y = Y.rename(columns={'W/L_A': 'Label'})\n",
    "    \n",
    "    # Get averaged attributes X\n",
    "    for idx, row in df.iterrows():\n",
    "        df_sel = df.loc[df.Date_A <= row['Date_A'], :].reset_index(drop=True)\n",
    "        \n",
    "        # Process of Team_A\n",
    "        gamePlayed_A = df_sel.loc[df_sel.Team_A == row['Team_A'], :]\n",
    "        if len(gamePlayed_A) == 1:\n",
    "            X_A = gamePlayed_A.loc[(gamePlayed_A.Team_A == row['Team_A']), :].sort_values(by=['Date_A'], ascending=False).iloc[0:1, 0:24].reset_index(drop=True)\n",
    "        elif len(gamePlayed_A) < period:\n",
    "            X_A = gamePlayed_A.loc[(gamePlayed_A.Team_A == row['Team_A']), :].sort_values(by=['Date_A'], ascending=False).iloc[1:len(gamePlayed_A), 0:24].reset_index(drop=True)\n",
    "        else:\n",
    "            X_A = gamePlayed_A.loc[(gamePlayed_A.Team_A == row['Team_A']), :].sort_values(by=['Date_A'], ascending=False).iloc[1:period+1, 0:24].reset_index(drop=True)\n",
    "        \n",
    "        # Process of Team_B\n",
    "        gamePlayed_B = df_sel.loc[df_sel.Team_A == row['Team_B'], :]\n",
    "        if len(gamePlayed_B) == 1:\n",
    "            X_B = gamePlayed_B.loc[(gamePlayed_B.Team_A == row['Team_B']), :].sort_values(by=['Date_A'], ascending=False).iloc[0:1, 0:24].reset_index(drop=True)\n",
    "        elif len(gamePlayed_B) < period:\n",
    "            X_B = gamePlayed_B.loc[(gamePlayed_B.Team_A == row['Team_B']), :].sort_values(by=['Date_A'], ascending=False).iloc[1:len(gamePlayed_B), 0:24].reset_index(drop=True)\n",
    "        else:\n",
    "            X_B = gamePlayed_B.loc[(gamePlayed_B.Team_A == row['Team_B']), :].sort_values(by=['Date_A'], ascending=False).iloc[1:period+1, 0:24].reset_index(drop=True)\n",
    "        \n",
    "        # Drop unnecessary attributes\n",
    "        colToDrop = ['Home/Away_A'] + ['Team_A', 'Date_A', 'W/L_A', 'Score_A', 'Opponent_A']\n",
    "        X_A = X_A.drop(columns=colToDrop)\n",
    "        X_B = X_B.drop(columns=colToDrop)\n",
    "        \n",
    "        # Rename X_B's columns\n",
    "        X_B = X_B.rename(columns=lambda x: x[0:-2] + '_B')\n",
    "        \n",
    "        # Get X_single = [Home/Away_A + X_A + X_B]\n",
    "        X_single = pd.DataFrame(data=pd.concat([X_A.mean(), X_B.mean()])).transpose()\n",
    "        X_single = pd.concat([pd.DataFrame(data={'Home/Away_A': [row['Home/Away_A']]}), X_single], axis=1)\n",
    "        \n",
    "        # Concatenation dataFrames by row\n",
    "        if idx == 0:\n",
    "            X = X_single\n",
    "        else:\n",
    "            X = pd.concat([X, X_single], ignore_index=True)\n",
    "        \n",
    "    # Feature Engineering\n",
    "    X = featureEng(X, featureSel)\n",
    "        \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CrossValidationGridSearchNested(X_data, Y_data, num_trials, fold_num, est_classifcation, tuned_param, scoring):\n",
    "    max_score = -1\n",
    "    best_estimator = est_classifcation\n",
    "    is_tuned_param_empty = (tuned_param == []) | (tuned_param == None)\n",
    "    \n",
    "    for i in range(num_trials):\n",
    "        inner_cv = StratifiedKFold(n_splits=fold_num, random_state=i, shuffle=True)\n",
    "        outer_cv = StratifiedKFold(n_splits=fold_num, random_state=i+1, shuffle=True)\n",
    "        \n",
    "        if(is_tuned_param_empty):\n",
    "            param_score = cross_val_score(est_classifcation, X=X_data, y=Y_data, cv=outer_cv, scoring=scoring).mean()\n",
    "        else:\n",
    "            # Non_nested parameter search and scoring\n",
    "            clf = GridSearchCV(estimator=est_classifcation, param_grid=tuned_param, cv=inner_cv, scoring=scoring)\n",
    "            clf.fit(X_data, Y_data)\n",
    "        \n",
    "            # CV with parameter optimization\n",
    "            param_score = cross_val_score(clf.best_estimator_, X=X_data, y=Y_data, cv=outer_cv, scoring=scoring).mean()\n",
    "            \n",
    "        if(param_score > max_score):\n",
    "            max_score = param_score\n",
    "            if(is_tuned_param_empty):\n",
    "                best_estimator = est_classifcation\n",
    "            else:\n",
    "                best_estimator = clf.best_estimator_\n",
    "            \n",
    "        progress = (i+1)/num_trials*100\n",
    "        print(f'> progress = {progress}%')\n",
    "    \n",
    "    return (max_score, best_estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> progress = 100.0%\n",
      "Execution time = 96.23733568191528\n"
     ]
    }
   ],
   "source": [
    "startTime = time.time()\n",
    "\n",
    "# Model Settings\n",
    "modelName = '_LogiRegr'\n",
    "model = LogisticRegression()\n",
    "tuned_parameters = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "    'max_iter': [100, 200, 300, 400, 500],\n",
    "    'n_jobs': [-1]\n",
    "}\n",
    "\n",
    "# Feature Extraction\n",
    "X, Y = featureExtraction(dfFile, dateStart, dateEnd, period, featureSel)\n",
    "\n",
    "# Number of random trials\n",
    "NUM_TRIALS = 1\n",
    "(max_score, best_estimator) = CrossValidationGridSearchNested(X, Y, NUM_TRIALS, 10, model, tuned_parameters, 'roc_auc')\n",
    "\n",
    "# Write to .csv\n",
    "with open('./00_model_param/' + trialName + modelName + '.csv','w') as myFile:\n",
    "    for key, value in zip(best_estimator.get_params().keys(), best_estimator.get_params().values()):\n",
    "        myFile.write(key + ',' + str(value) + '\\n')\n",
    "    myFile.write('max_score' + ',' + str(max_score) + '\\n')\n",
    "    myFile.write('Execution time =' + ',' + str(time.time() - startTime) + '\\n')\n",
    "\n",
    "print('Execution time =', time.time() - startTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> progress = 100.0%\n",
      "Execution time = 10890.471765041351\n"
     ]
    }
   ],
   "source": [
    "startTime = time.time()\n",
    "\n",
    "# Model Settings\n",
    "modelName = '_SVM'\n",
    "model = SVC()\n",
    "tuned_parameters = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'kernel': ['linear'], \n",
    "    'probability': [True]\n",
    "}\n",
    "\n",
    "# Feature Extraction\n",
    "X, Y = featureExtraction(dfFile, dateStart, dateEnd, period, featureSel)\n",
    "\n",
    "# Number of random trials\n",
    "NUM_TRIALS = 1\n",
    "(max_score, best_estimator) = CrossValidationGridSearchNested(X, Y, NUM_TRIALS, 10, model, tuned_parameters, 'roc_auc')\n",
    "\n",
    "# Write to .csv\n",
    "with open('./00_model_param/' + trialName + modelName + '.csv','w') as myFile:\n",
    "    for key, value in zip(best_estimator.get_params().keys(), best_estimator.get_params().values()):\n",
    "        myFile.write(key + ',' + str(value) + '\\n')\n",
    "    myFile.write('max_score' + ',' + str(max_score) + '\\n')\n",
    "    myFile.write('Execution time =' + ',' + str(time.time() - startTime) + '\\n')\n",
    "\n",
    "print('Execution time =', time.time() - startTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> progress = 100.0%\n",
      "Execution time = 1336.421709060669\n"
     ]
    }
   ],
   "source": [
    "startTime = time.time()\n",
    "\n",
    "# Model Settings\n",
    "modelName = '_XGBoost'\n",
    "model = XGBClassifier()\n",
    "tuned_parameters = {\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [x/10 for x in range(1, 5, 2)],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'min_child_weight': [1, 3],\n",
    "    'gamma': [x/10 for x in range(0, 5)],\n",
    "    'n_jobs': [-1]\n",
    "}\n",
    "\n",
    "# Feature Extraction\n",
    "X, Y = featureExtraction(dfFile, dateStart, dateEnd, period, featureSel)\n",
    "\n",
    "# Number of random trials\n",
    "NUM_TRIALS = 1\n",
    "(max_score, best_estimator) = CrossValidationGridSearchNested(X, Y, NUM_TRIALS, 10, model, tuned_parameters, 'roc_auc')\n",
    "\n",
    "# Write to .csv\n",
    "with open('./00_model_param/' + trialName + modelName + '.csv','w') as myFile:\n",
    "    for key, value in zip(best_estimator.get_params().keys(), best_estimator.get_params().values()):\n",
    "        myFile.write(key + ',' + str(value) + '\\n')\n",
    "    myFile.write('max_score' + ',' + str(max_score) + '\\n')\n",
    "    myFile.write('Execution time =' + ',' + str(time.time() - startTime) + '\\n')\n",
    "\n",
    "print('Execution time =', time.time() - startTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> progress = 100.0%\n",
      "Execution time = 1110.9388637542725\n"
     ]
    }
   ],
   "source": [
    "startTime = time.time()\n",
    "\n",
    "# Model Settings\n",
    "modelName = '_RandomForest'\n",
    "model = RandomForestClassifier()\n",
    "tuned_parameters = {\n",
    "    'n_estimators': [600, 800, 1000],\n",
    "    'criterion': ['entropy'],\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [None, 5, 10],\n",
    "    'max_features': ['auto', 'log2', 'sqrt'],\n",
    "    'n_jobs': [-1]\n",
    "}\n",
    "\n",
    "# Feature Extraction\n",
    "X, Y = featureExtraction(dfFile, dateStart, dateEnd, period, featureSel)\n",
    "\n",
    "# Number of random trials\n",
    "NUM_TRIALS = 1\n",
    "(max_score, best_estimator) = CrossValidationGridSearchNested(X, Y, NUM_TRIALS, 10, model, tuned_parameters, 'roc_auc')\n",
    "\n",
    "# Write to .csv\n",
    "with open('./00_model_param/' + trialName + modelName + '.csv','w') as myFile:\n",
    "    for key, value in zip(best_estimator.get_params().keys(), best_estimator.get_params().values()):\n",
    "        myFile.write(key + ',' + str(value) + '\\n')\n",
    "    myFile.write('max_score' + ',' + str(max_score) + '\\n')\n",
    "    myFile.write('Execution time =' + ',' + str(time.time() - startTime) + '\\n')\n",
    "\n",
    "print('Execution time =', time.time() - startTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> progress = 100.0%\n",
      "Execution time = 4611.803181886673\n"
     ]
    }
   ],
   "source": [
    "startTime = time.time()\n",
    "\n",
    "# Model Settings\n",
    "modelName = '_GBDT'\n",
    "model = GradientBoostingClassifier()\n",
    "tuned_parameters = {\n",
    "    'loss': ['exponential'],\n",
    "    'n_estimators': [600, 800, 1000],\n",
    "    'learning_rate': [0.1, 0.2, 0.3],\n",
    "    'max_depth': [3, 5, 10],\n",
    "    'subsample': [0.5],\n",
    "    'max_features': ['auto', 'log2', 'sqrt']\n",
    "}\n",
    "\n",
    "# Feature Extraction\n",
    "X, Y = featureExtraction(dfFile, dateStart, dateEnd, period, featureSel)\n",
    "\n",
    "# Number of random trials\n",
    "NUM_TRIALS = 1\n",
    "(max_score, best_estimator) = CrossValidationGridSearchNested(X, Y, NUM_TRIALS, 10, model, tuned_parameters, 'roc_auc')\n",
    "\n",
    "# Write to .csv\n",
    "with open('./00_model_param/' + trialName + modelName + '.csv','w') as myFile:\n",
    "    for key, value in zip(best_estimator.get_params().keys(), best_estimator.get_params().values()):\n",
    "        myFile.write(key + ',' + str(value) + '\\n')\n",
    "    myFile.write('max_score' + ',' + str(max_score) + '\\n')\n",
    "    myFile.write('Execution time =' + ',' + str(time.time() - startTime) + '\\n')\n",
    "\n",
    "print('Execution time =', time.time() - startTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> progress = 100.0%\n",
      "Execution time = 2567.632943868637\n"
     ]
    }
   ],
   "source": [
    "startTime = time.time()\n",
    "\n",
    "# Model Settings\n",
    "modelName = '_LightGBM'\n",
    "model = LGBMClassifier()\n",
    "tuned_parameters = {\n",
    "    'learning_rate': [0.1, 0.2, 0.3],\n",
    "    'n_estimators': [600, 800, 1000],\n",
    "    'max_depth': [-1, 5, 10],\n",
    "    'subsample' : [0.5]\n",
    "}\n",
    "\n",
    "# Feature Extraction\n",
    "X, Y = featureExtraction(dfFile, dateStart, dateEnd, period, featureSel)\n",
    "\n",
    "# Number of random trials\n",
    "NUM_TRIALS = 1\n",
    "(max_score, best_estimator) = CrossValidationGridSearchNested(X, Y, NUM_TRIALS, 10, model, tuned_parameters, 'roc_auc')\n",
    "\n",
    "# Write to .csv\n",
    "with open('./00_model_param/' + trialName + modelName + '.csv','w') as myFile:\n",
    "    for key, value in zip(best_estimator.get_params().keys(), best_estimator.get_params().values()):\n",
    "        myFile.write(key + ',' + str(value) + '\\n')\n",
    "    myFile.write('max_score' + ',' + str(max_score) + '\\n')\n",
    "    myFile.write('Execution time =' + ',' + str(time.time() - startTime) + '\\n')\n",
    "\n",
    "print('Execution time =', time.time() - startTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> progress = 100.0%\n",
      "Execution time = 837.9147560596466\n"
     ]
    }
   ],
   "source": [
    "startTime = time.time()\n",
    "\n",
    "# Model Settings\n",
    "modelName = '_AdaBoost'\n",
    "model = AdaBoostClassifier()\n",
    "tuned_parameters = {\n",
    "    'learning_rate': [1, 0.1, 0.2, 0.3],\n",
    "    'n_estimators': [50, 100, 600, 800, 1000],\n",
    "}\n",
    "\n",
    "# Feature Extraction\n",
    "X, Y = featureExtraction(dfFile, dateStart, dateEnd, period, featureSel)\n",
    "\n",
    "# Number of random trials\n",
    "NUM_TRIALS = 1\n",
    "(max_score, best_estimator) = CrossValidationGridSearchNested(X, Y, NUM_TRIALS, 10, model, tuned_parameters, 'roc_auc')\n",
    "\n",
    "# Write to .csv\n",
    "with open('./00_model_param/' + trialName + modelName + '.csv','w') as myFile:\n",
    "    for key, value in zip(best_estimator.get_params().keys(), best_estimator.get_params().values()):\n",
    "        myFile.write(key + ',' + str(value) + '\\n')\n",
    "    myFile.write('max_score' + ',' + str(max_score) + '\\n')\n",
    "    myFile.write('Execution time =' + ',' + str(time.time() - startTime) + '\\n')\n",
    "\n",
    "print('Execution time =', time.time() - startTime)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
