{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# @param dfFile: pandas.DataFrame ('nba_preprocessed.csv')\n",
    "# @param dateStart, dateEnd: str in the format of 'YYYY-MM-DD'\n",
    "# @param attriToDrop: list[str]\n",
    "# @return X, Y: pandas.DataFrame\n",
    "# featureExtraction() outputs X, Y for model training.\n",
    "# Game date can be assigned\n",
    "# Attribute to be dropped can be assigned\n",
    "def featureExtraction(dfFile, dateStart='1000-01-01', dateEnd='2999-12-31', attriToDrop=None):\n",
    "    df = pd.read_csv(dfFile)\n",
    "    \n",
    "    # Date selection\n",
    "    df = df.loc[lambda df: (df.Date_A > dateStart) & (df.Date_A < dateEnd), :].reset_index(drop=True)\n",
    "    \n",
    "    # Get label Y\n",
    "    Y = df[['W/L_A']]\n",
    "    Y = Y.rename(columns={'W/L_A': 'Label'})\n",
    "    \n",
    "    # Get attributes X\n",
    "    colToDrop = ['Team_A', 'Date_A', 'W/L_A', 'Score_A', 'Opponent_A', 'Team_B', 'Date_B', 'W/L_B', 'Home/Away_B', 'Score_B', 'Opponent_B']\n",
    "    colToDrop += attriToDrop if attriToDrop else []\n",
    "    X = df.drop(columns = colToDrop)\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "import operator\n",
    "%matplotlib inline\n",
    "\n",
    "def CrossValidationGridSearchNested(X_data, Y_data, num_trials, fold_num, est_classifcation, tuned_param, scoring):\n",
    "    max_score = -1\n",
    "    best_estimator = est_classifcation\n",
    "    \n",
    "    for i in range(num_trials):\n",
    "        inner_cv = StratifiedKFold(n_splits=fold_num, random_state=i, shuffle=True)\n",
    "        outer_cv = StratifiedKFold(n_splits=fold_num, random_state=i+1, shuffle=True)\n",
    "\n",
    "        # Non_nested parameter search and scoring\n",
    "        clf = GridSearchCV(estimator=est_classifcation, param_grid=tuned_param, cv=inner_cv, scoring=scoring)\n",
    "        clf.fit(X_data, Y_data)\n",
    "        \n",
    "        # CV with parameter optimization\n",
    "        param_score = cross_val_score(clf.best_estimator_, X=X_data, y=Y_data, cv=outer_cv, scoring=scoring).mean()\n",
    "        if(param_score > max_score):\n",
    "            max_score = param_score\n",
    "            best_estimator = clf.best_estimator_\n",
    "            \n",
    "        progress = (i+1)/num_trials*100\n",
    "        print(f'> progress = {progress}%')\n",
    "    \n",
    "    return (max_score, best_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFile = '../NBA_Ino_part/nba_preprocessed.csv'\n",
    "dateStart = '2017-10-01'\n",
    "dateEnd = '2018-04-30'\n",
    "# X, Y = featureExtraction(dfFile, dateStart, dateEnd)\n",
    "X, Y = featureExtraction(dfFile, attriToDrop=['PTS_A', 'PTS_B'], dateStart=dateStart, dateEnd=dateEnd)\n",
    "# X, Y = featureExtraction(dfFile)\n",
    "X_val = X.values\n",
    "Y_val = Y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> progress = 100.0%\n",
      "CrossValidationGridSearchNested of LightGradientBoostingClassifier wih NUM_TRIALS =  1 took 56815.07 seconds.\n",
      "\n",
      "max_score = 0.9872893119175095\n",
      "\n",
      "\n",
      "best_estimator = LGBMClassifier(boosting_type='gbdt', cat_smooth=10, class_weight=None,\n",
      "        colsample_bytree=0.65, learning_rate=0.1, max_bin=512,\n",
      "        max_depth=11, metric='auc', min_child_samples=5,\n",
      "        min_child_weight=1, min_split_gain=0.5, n_estimators=500,\n",
      "        n_jobs=-1, num_leaves=31, objective='binary', random_state=None,\n",
      "        reg_alpha=5, reg_lambda=10, scale_pos_weight=1, silent=True,\n",
      "        subsample=0.5, subsample_for_bin=200, subsample_freq=1)\n",
      "\n",
      "\n",
      "best_parameter = {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 0.65, 'learning_rate': 0.1, 'max_depth': 11, 'min_child_samples': 5, 'min_child_weight': 1, 'min_split_gain': 0.5, 'n_estimators': 500, 'n_jobs': -1, 'num_leaves': 31, 'objective': 'binary', 'random_state': None, 'reg_alpha': 5, 'reg_lambda': 10, 'silent': True, 'subsample': 0.5, 'subsample_for_bin': 200, 'subsample_freq': 1, 'max_bin': 512, 'scale_pos_weight': 1, 'cat_smooth': 10, 'metric': 'auc'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb  \n",
    "from lightgbm import LGBMClassifier \n",
    "from time import time\n",
    "\n",
    "params = {'boosting_type': 'gbdt',\n",
    "          'max_depth' : -1,\n",
    "          'objective': 'binary', \n",
    "          'nthread': 5, # Updated from nthread\n",
    "          'num_leaves': 64, \n",
    "          'learning_rate': 0.05, \n",
    "          'max_bin': 512, \n",
    "          'subsample_for_bin': 200,\n",
    "          'subsample': 1, \n",
    "          'subsample_freq': 1, \n",
    "          'colsample_bytree': 0.8, \n",
    "          'reg_alpha': 5, \n",
    "          'reg_lambda': 10,\n",
    "          'min_split_gain': 0.5, \n",
    "          'min_child_weight': 1, \n",
    "          'min_child_samples': 5, \n",
    "          'scale_pos_weight': 1,\n",
    "          'num_class' : 1,\n",
    "          'metric' : 'binary_error',\n",
    "          'cat_smooth' : 10,\n",
    "          'objective' : 'binary',\n",
    "          'metric' : 'auc'}\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = {\n",
    "    'learning_rate': [0.1],\n",
    "    'n_estimators': range(100, 1001, 400),\n",
    "    \"max_depth\": [11, 12],\n",
    "    #'num_leaves': range(30, 100, 30),\n",
    "    'boosting_type' : ['gbdt'],\n",
    "    #'random_state' : [501], # Updated from 'seed'\n",
    "    'colsample_bytree' : [0.65],\n",
    "    'subsample' : [0.5]\n",
    "    #'reg_alpha' : [1, 1.2],\n",
    "    #'reg_lambda' : [1, 1.2, 1.4],\n",
    "    }\n",
    "\n",
    "# Number of random trials\n",
    "NUM_TRIALS = 1\n",
    "\n",
    "# We will use a Support Vector Classifier with \"rbf\" kernel\n",
    "#lgbm = LGBMClassifier()\n",
    "lgbm = LGBMClassifier(boosting_type= 'gbdt',  \n",
    "          n_jobs = -1, # Updated from 'nthread'\n",
    "          silent = True,\n",
    "          max_depth = params['max_depth'],\n",
    "          max_bin = params['max_bin'], \n",
    "          subsample_for_bin = params['subsample_for_bin'],\n",
    "          subsample = params['subsample'], \n",
    "          subsample_freq = params['subsample_freq'], \n",
    "          min_split_gain = params['min_split_gain'], \n",
    "          min_child_weight = params['min_child_weight'], \n",
    "          min_child_samples = params['min_child_samples'], \n",
    "          scale_pos_weight = params['scale_pos_weight'],\n",
    "          cat_smooth = params['cat_smooth'],\n",
    "          objective = params['objective'],\n",
    "          reg_alpha = params['reg_alpha'],\n",
    "          reg_lambda = params['reg_lambda'],\n",
    "          metric = params['metric'])\n",
    "\n",
    "start = time()\n",
    "(max_score, lgbm_best_estimator) = CrossValidationGridSearchNested(X_val, Y_val.ravel(), NUM_TRIALS, 10, lgbm, tuned_parameters, 'roc_auc')\n",
    "lgbm_best_parameter = lgbm_best_estimator.get_params()\n",
    "\n",
    "print(\"CrossValidationGridSearchNested of LightGradientBoostingClassifier wih NUM_TRIALS = %2.0d took %.2f seconds.\"%(NUM_TRIALS, (time() - start)))\n",
    "print(f'\\nmax_score = {max_score}\\n')\n",
    "print(f'\\nbest_estimator = {lgbm_best_estimator}\\n')\n",
    "print(f'\\nbest_parameter = {lgbm_best_parameter}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> progress = 100.0%\n",
      "CrossValidationGridSearchNested of LightGradientBoostingClassifier wih NUM_TRIALS =  1 took 96357.52 seconds.\n",
      "\n",
      "max_score = 0.9878247075153679\n",
      "\n",
      "\n",
      "best_estimator = LGBMClassifier(boosting_type='gbdt', cat_smooth=10, class_weight=None,\n",
      "        colsample_bytree=0.65, learning_rate=0.05, max_bin=512,\n",
      "        max_depth=8, metric='auc', min_child_samples=5, min_child_weight=1,\n",
      "        min_split_gain=0.5, n_estimators=500, n_jobs=-1, num_leaves=31,\n",
      "        objective='binary', random_state=None, reg_alpha=5, reg_lambda=10,\n",
      "        scale_pos_weight=1, silent=True, subsample=0.5,\n",
      "        subsample_for_bin=200, subsample_freq=1)\n",
      "\n",
      "\n",
      "best_parameter = {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 0.65, 'learning_rate': 0.05, 'max_depth': 8, 'min_child_samples': 5, 'min_child_weight': 1, 'min_split_gain': 0.5, 'n_estimators': 500, 'n_jobs': -1, 'num_leaves': 31, 'objective': 'binary', 'random_state': None, 'reg_alpha': 5, 'reg_lambda': 10, 'silent': True, 'subsample': 0.5, 'subsample_for_bin': 200, 'subsample_freq': 1, 'max_bin': 512, 'scale_pos_weight': 1, 'cat_smooth': 10, 'metric': 'auc'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb  \n",
    "from lightgbm import LGBMClassifier \n",
    "from time import time\n",
    "\n",
    "params = {'boosting_type': 'gbdt',\n",
    "          'max_depth' : -1,\n",
    "          'objective': 'binary', \n",
    "          'nthread': 5, # Updated from nthread\n",
    "          'num_leaves': 64, \n",
    "          'learning_rate': 0.05, \n",
    "          'max_bin': 512, \n",
    "          'subsample_for_bin': 200,\n",
    "          'subsample': 1, \n",
    "          'subsample_freq': 1, \n",
    "          'colsample_bytree': 0.8, \n",
    "          'reg_alpha': 5, \n",
    "          'reg_lambda': 10,\n",
    "          'min_split_gain': 0.5, \n",
    "          'min_child_weight': 1, \n",
    "          'min_child_samples': 5, \n",
    "          'scale_pos_weight': 1,\n",
    "          'num_class' : 1,\n",
    "          'metric' : 'binary_error',\n",
    "          'cat_smooth' : 10,\n",
    "          'objective' : 'binary',\n",
    "          'metric' : 'auc'}\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = {\n",
    "    #'learning_rate': [2, 1.5, 1, 0.5, 0.25, 0.1, 0.05, 0.01, 0.005, 0.001],\n",
    "    'learning_rate': [0.05],\n",
    "    'n_estimators': [500],\n",
    "    \"max_depth\": [3, 5, 8, 9, 11, 12],\n",
    "    #'num_leaves': range(30, 100, 30),\n",
    "    'boosting_type' : ['gbdt'],\n",
    "    #'random_state' : [501], # Updated from 'seed'\n",
    "    'colsample_bytree' : [0.65],\n",
    "    'subsample' : [0.5]\n",
    "    #'reg_alpha' : [1, 1.2],\n",
    "    #'reg_lambda' : [1, 1.2, 1.4],\n",
    "    }\n",
    "\n",
    "# Number of random trials\n",
    "NUM_TRIALS = 1\n",
    "\n",
    "# We will use a Support Vector Classifier with \"rbf\" kernel\n",
    "#lgbm = LGBMClassifier()\n",
    "lgbm = LGBMClassifier(boosting_type= 'gbdt',  \n",
    "          n_jobs = -1, # Updated from 'nthread'\n",
    "          silent = True,\n",
    "          max_depth = params['max_depth'],\n",
    "          max_bin = params['max_bin'], \n",
    "          subsample_for_bin = params['subsample_for_bin'],\n",
    "          subsample = params['subsample'], \n",
    "          subsample_freq = params['subsample_freq'], \n",
    "          min_split_gain = params['min_split_gain'], \n",
    "          min_child_weight = params['min_child_weight'], \n",
    "          min_child_samples = params['min_child_samples'], \n",
    "          scale_pos_weight = params['scale_pos_weight'],\n",
    "          cat_smooth = params['cat_smooth'],\n",
    "          objective = params['objective'],\n",
    "          reg_alpha = params['reg_alpha'],\n",
    "          reg_lambda = params['reg_lambda'],\n",
    "          metric = params['metric'])\n",
    "\n",
    "start = time()\n",
    "(max_score, lgbm_best_estimator) = CrossValidationGridSearchNested(X_val, Y_val.ravel(), NUM_TRIALS, 10, lgbm, tuned_parameters, 'roc_auc')\n",
    "lgbm_best_parameter = lgbm_best_estimator.get_params()\n",
    "\n",
    "print(\"CrossValidationGridSearchNested of LightGradientBoostingClassifier wih NUM_TRIALS = %2.0d took %.2f seconds.\"%(NUM_TRIALS, (time() - start)))\n",
    "print(f'\\nmax_score = {max_score}\\n')\n",
    "print(f'\\nbest_estimator = {lgbm_best_estimator}\\n')\n",
    "print(f'\\nbest_parameter = {lgbm_best_parameter}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> progress = 100.0%\n",
      "CrossValidationGridSearchNested of LightGradientBoostingClassifier wih NUM_TRIALS =  1 took 61402.38 seconds.\n",
      "\n",
      "max_score = 0.9878247075153679\n",
      "\n",
      "\n",
      "best_estimator = LGBMClassifier(boosting_type='gbdt', cat_smooth=10, class_weight=None,\n",
      "        colsample_bytree=0.65, learning_rate=0.05, max_bin=512,\n",
      "        max_depth=8, metric='auc', min_child_samples=5, min_child_weight=1,\n",
      "        min_split_gain=0.5, n_estimators=500, n_jobs=-1, num_leaves=31,\n",
      "        objective='binary', random_state=None, reg_alpha=5, reg_lambda=10,\n",
      "        scale_pos_weight=1, silent=True, subsample=0.5,\n",
      "        subsample_for_bin=200, subsample_freq=1)\n",
      "\n",
      "\n",
      "best_parameter = {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 0.65, 'learning_rate': 0.05, 'max_depth': 8, 'min_child_samples': 5, 'min_child_weight': 1, 'min_split_gain': 0.5, 'n_estimators': 500, 'n_jobs': -1, 'num_leaves': 31, 'objective': 'binary', 'random_state': None, 'reg_alpha': 5, 'reg_lambda': 10, 'silent': True, 'subsample': 0.5, 'subsample_for_bin': 200, 'subsample_freq': 1, 'max_bin': 512, 'scale_pos_weight': 1, 'cat_smooth': 10, 'metric': 'auc'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = {\n",
    "    #'learning_rate': [2, 1.5, 1, 0.5, 0.25, 0.1, 0.05, 0.01, 0.005, 0.001],\n",
    "    'learning_rate': [0.05],\n",
    "    'n_estimators': [500],\n",
    "    \"max_depth\": [8],\n",
    "    'boosting_type' : ['gbdt'],\n",
    "    #'random_state' : [501], # Updated from 'seed'\n",
    "    'colsample_bytree' : [0.1, 0.65, 0.8, 1],\n",
    "    'subsample' : [0.5]\n",
    "    #'reg_alpha' : [1, 1.2],\n",
    "    #'reg_lambda' : [1, 1.2, 1.4],\n",
    "    }\n",
    "\n",
    "# Number of random trials\n",
    "NUM_TRIALS = 1\n",
    "\n",
    "# We will use a Support Vector Classifier with \"rbf\" kernel\n",
    "#lgbm = LGBMClassifier()\n",
    "lgbm = LGBMClassifier(boosting_type= 'gbdt',  \n",
    "          n_jobs = -1, # Updated from 'nthread'\n",
    "          silent = True,\n",
    "          max_depth = params['max_depth'],\n",
    "          max_bin = params['max_bin'], \n",
    "          subsample_for_bin = params['subsample_for_bin'],\n",
    "          subsample = params['subsample'], \n",
    "          subsample_freq = params['subsample_freq'], \n",
    "          min_split_gain = params['min_split_gain'], \n",
    "          min_child_weight = params['min_child_weight'], \n",
    "          min_child_samples = params['min_child_samples'], \n",
    "          scale_pos_weight = params['scale_pos_weight'],\n",
    "          cat_smooth = params['cat_smooth'],\n",
    "          objective = params['objective'],\n",
    "          reg_alpha = params['reg_alpha'],\n",
    "          reg_lambda = params['reg_lambda'],\n",
    "          metric = params['metric'])\n",
    "\n",
    "start = time()\n",
    "(max_score, lgbm_best_estimator) = CrossValidationGridSearchNested(X_val, Y_val.ravel(), NUM_TRIALS, 10, lgbm, tuned_parameters, 'roc_auc')\n",
    "lgbm_best_parameter = lgbm_best_estimator.get_params()\n",
    "\n",
    "print(\"CrossValidationGridSearchNested of LightGradientBoostingClassifier wih NUM_TRIALS = %2.0d took %.2f seconds.\"%(NUM_TRIALS, (time() - start)))\n",
    "print(f'\\nmax_score = {max_score}\\n')\n",
    "print(f'\\nbest_estimator = {lgbm_best_estimator}\\n')\n",
    "print(f'\\nbest_parameter = {lgbm_best_parameter}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> progress = 100.0%\n",
      "CrossValidationGridSearchNested of LightGradientBoostingClassifier wih NUM_TRIALS =  1 took 55809.89 seconds.\n",
      "\n",
      "max_score = 0.9878247075153679\n",
      "\n",
      "\n",
      "best_estimator = LGBMClassifier(boosting_type='gbdt', cat_smooth=10, class_weight=None,\n",
      "        colsample_bytree=0.65, learning_rate=0.05, max_bin=512,\n",
      "        max_depth=8, metric='auc', min_child_samples=5, min_child_weight=1,\n",
      "        min_split_gain=0.5, n_estimators=500, n_jobs=-1, num_leaves=31,\n",
      "        objective='binary', random_state=None, reg_alpha=5, reg_lambda=10,\n",
      "        scale_pos_weight=1, silent=True, subsample=0.5,\n",
      "        subsample_for_bin=200, subsample_freq=1)\n",
      "\n",
      "\n",
      "best_parameter = {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 0.65, 'learning_rate': 0.05, 'max_depth': 8, 'min_child_samples': 5, 'min_child_weight': 1, 'min_split_gain': 0.5, 'n_estimators': 500, 'n_jobs': -1, 'num_leaves': 31, 'objective': 'binary', 'random_state': None, 'reg_alpha': 5, 'reg_lambda': 10, 'silent': True, 'subsample': 0.5, 'subsample_for_bin': 200, 'subsample_freq': 1, 'max_bin': 512, 'scale_pos_weight': 1, 'cat_smooth': 10, 'metric': 'auc'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = {\n",
    "    #'learning_rate': [2, 1.5, 1, 0.5, 0.25, 0.1, 0.05, 0.01, 0.005, 0.001],\n",
    "    'learning_rate': [0.05],\n",
    "    'n_estimators': [500],\n",
    "    \"max_depth\": [8],\n",
    "    'boosting_type' : ['gbdt'],\n",
    "    #'random_state' : [501], # Updated from 'seed'\n",
    "    'colsample_bytree' : [0.65],\n",
    "    'subsample' : [0.1, 0.5, 0.75, 1]\n",
    "    #'reg_alpha' : [1, 1.2],\n",
    "    #'reg_lambda' : [1, 1.2, 1.4],\n",
    "    }\n",
    "\n",
    "# Number of random trials\n",
    "NUM_TRIALS = 1\n",
    "\n",
    "# We will use a Support Vector Classifier with \"rbf\" kernel\n",
    "#lgbm = LGBMClassifier()\n",
    "lgbm = LGBMClassifier(boosting_type= 'gbdt',  \n",
    "          n_jobs = -1, # Updated from 'nthread'\n",
    "          silent = True,\n",
    "          max_depth = params['max_depth'],\n",
    "          max_bin = params['max_bin'], \n",
    "          subsample_for_bin = params['subsample_for_bin'],\n",
    "          subsample = params['subsample'], \n",
    "          subsample_freq = params['subsample_freq'], \n",
    "          min_split_gain = params['min_split_gain'], \n",
    "          min_child_weight = params['min_child_weight'], \n",
    "          min_child_samples = params['min_child_samples'], \n",
    "          scale_pos_weight = params['scale_pos_weight'],\n",
    "          cat_smooth = params['cat_smooth'],\n",
    "          objective = params['objective'],\n",
    "          reg_alpha = params['reg_alpha'],\n",
    "          reg_lambda = params['reg_lambda'],\n",
    "          metric = params['metric'])\n",
    "\n",
    "start = time()\n",
    "(max_score, lgbm_best_estimator) = CrossValidationGridSearchNested(X_val, Y_val.ravel(), NUM_TRIALS, 10, lgbm, tuned_parameters, 'roc_auc')\n",
    "lgbm_best_parameter = lgbm_best_estimator.get_params()\n",
    "\n",
    "print(\"CrossValidationGridSearchNested of LightGradientBoostingClassifier wih NUM_TRIALS = %2.0d took %.2f seconds.\"%(NUM_TRIALS, (time() - start)))\n",
    "print(f'\\nmax_score = {max_score}\\n')\n",
    "print(f'\\nbest_estimator = {lgbm_best_estimator}\\n')\n",
    "print(f'\\nbest_parameter = {lgbm_best_parameter}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> progress = 100.0%\n",
      "CrossValidationGridSearchNested of LightGradientBoostingClassifier wih NUM_TRIALS =  1 took 115888.47 seconds.\n",
      "\n",
      "max_score = 0.9919161874545577\n",
      "\n",
      "\n",
      "best_estimator = LGBMClassifier(boosting_type='gbdt', cat_smooth=10, class_weight=None,\n",
      "        colsample_bytree=0.65, learning_rate=0.1, max_bin=512, max_depth=8,\n",
      "        metric='auc', min_child_samples=5, min_child_weight=1,\n",
      "        min_split_gain=0.5, n_estimators=500, n_jobs=-1, num_leaves=31,\n",
      "        objective='binary', random_state=None, reg_alpha=1e-05,\n",
      "        reg_lambda=10, scale_pos_weight=1, silent=True, subsample=0.5,\n",
      "        subsample_for_bin=200, subsample_freq=1)\n",
      "\n",
      "\n",
      "best_parameter = {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 0.65, 'learning_rate': 0.1, 'max_depth': 8, 'min_child_samples': 5, 'min_child_weight': 1, 'min_split_gain': 0.5, 'n_estimators': 500, 'n_jobs': -1, 'num_leaves': 31, 'objective': 'binary', 'random_state': None, 'reg_alpha': 1e-05, 'reg_lambda': 10, 'silent': True, 'subsample': 0.5, 'subsample_for_bin': 200, 'subsample_freq': 1, 'max_bin': 512, 'scale_pos_weight': 1, 'cat_smooth': 10, 'metric': 'auc'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb  \n",
    "from lightgbm import LGBMClassifier \n",
    "from time import time\n",
    "\n",
    "\n",
    "params = {'boosting_type': 'gbdt',\n",
    "          'max_depth' : -1,\n",
    "          'objective': 'binary', \n",
    "          'nthread': 5, # Updated from nthread\n",
    "          'num_leaves': 64, \n",
    "          'learning_rate': 0.05, \n",
    "          'max_bin': 512, \n",
    "          'subsample_for_bin': 200,\n",
    "          'subsample': 1, \n",
    "          'subsample_freq': 1, \n",
    "          'colsample_bytree': 0.8, \n",
    "          'reg_alpha': 5, \n",
    "          'reg_lambda': 10,\n",
    "          'min_split_gain': 0.5, \n",
    "          'min_child_weight': 1, \n",
    "          'min_child_samples': 5, \n",
    "          'scale_pos_weight': 1,\n",
    "          'num_class' : 1,\n",
    "          'metric' : 'binary_error',\n",
    "          'cat_smooth' : 10,\n",
    "          'objective' : 'binary',\n",
    "          'metric' : 'auc'}\n",
    "\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = {\n",
    "    'learning_rate': [2, 1.5, 1, 0.5, 0.25, 0.1, 0.05, 0.01, 0.005, 0.001],\n",
    "    #'learning_rate': [0.05],\n",
    "    'n_estimators': [500],\n",
    "    \"max_depth\": [8],\n",
    "    'boosting_type' : ['gbdt'],\n",
    "    #'random_state' : [501], # Updated from 'seed'\n",
    "    'colsample_bytree' : [0.65],\n",
    "    'subsample' : [0.5],\n",
    "    'reg_alpha' : [1e-5, 1e-2, 0.1, 1, 100, 1000]\n",
    "    #'reg_lambda' : [1, 1.2, 1.4],\n",
    "    }\n",
    "\n",
    "# Number of random trials\n",
    "NUM_TRIALS = 1\n",
    "\n",
    "# We will use a Support Vector Classifier with \"rbf\" kernel\n",
    "#lgbm = LGBMClassifier()\n",
    "lgbm = LGBMClassifier(boosting_type= 'gbdt',  \n",
    "          n_jobs = -1, # Updated from 'nthread'\n",
    "          silent = True,\n",
    "          max_depth = params['max_depth'],\n",
    "          max_bin = params['max_bin'], \n",
    "          subsample_for_bin = params['subsample_for_bin'],\n",
    "          subsample = params['subsample'], \n",
    "          subsample_freq = params['subsample_freq'], \n",
    "          min_split_gain = params['min_split_gain'], \n",
    "          min_child_weight = params['min_child_weight'], \n",
    "          min_child_samples = params['min_child_samples'], \n",
    "          scale_pos_weight = params['scale_pos_weight'],\n",
    "          cat_smooth = params['cat_smooth'],\n",
    "          objective = params['objective'],\n",
    "          reg_alpha = params['reg_alpha'],\n",
    "          reg_lambda = params['reg_lambda'],\n",
    "          metric = params['metric'])\n",
    "\n",
    "start = time()\n",
    "(max_score, lgbm_best_estimator) = CrossValidationGridSearchNested(X_val, Y_val.ravel(), NUM_TRIALS, 10, lgbm, tuned_parameters, 'roc_auc')\n",
    "lgbm_best_parameter = lgbm_best_estimator.get_params()\n",
    "\n",
    "print(\"CrossValidationGridSearchNested of LightGradientBoostingClassifier wih NUM_TRIALS = %2.0d took %.2f seconds.\"%(NUM_TRIALS, (time() - start)))\n",
    "print(f'\\nmax_score = {max_score}\\n')\n",
    "print(f'\\nbest_estimator = {lgbm_best_estimator}\\n')\n",
    "print(f'\\nbest_parameter = {lgbm_best_parameter}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> progress = 100.0%\n",
      "CrossValidationGridSearchNested of LightGradientBoostingClassifier wih NUM_TRIALS =  1 took 156517.27 seconds.\n",
      "\n",
      "max_score = 0.9919161874545577\n",
      "\n",
      "\n",
      "best_estimator = LGBMClassifier(boosting_type='gbdt', cat_smooth=10, class_weight=None,\n",
      "        colsample_bytree=0.65, learning_rate=0.1, max_bin=512, max_depth=8,\n",
      "        metric='auc', min_child_samples=5, min_child_weight=1,\n",
      "        min_split_gain=0.5, n_estimators=500, n_jobs=-1, num_leaves=31,\n",
      "        objective='binary', random_state=None, reg_alpha=1e-05,\n",
      "        reg_lambda=10, scale_pos_weight=1, silent=True, subsample=0.5,\n",
      "        subsample_for_bin=200, subsample_freq=1)\n",
      "\n",
      "\n",
      "best_parameter = {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 0.65, 'learning_rate': 0.1, 'max_depth': 8, 'min_child_samples': 5, 'min_child_weight': 1, 'min_split_gain': 0.5, 'n_estimators': 500, 'n_jobs': -1, 'num_leaves': 31, 'objective': 'binary', 'random_state': None, 'reg_alpha': 1e-05, 'reg_lambda': 10, 'silent': True, 'subsample': 0.5, 'subsample_for_bin': 200, 'subsample_freq': 1, 'max_bin': 512, 'scale_pos_weight': 1, 'cat_smooth': 10, 'metric': 'auc'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb  \n",
    "from lightgbm import LGBMClassifier \n",
    "from time import time\n",
    "\n",
    "\n",
    "params = {'boosting_type': 'gbdt',\n",
    "          'max_depth' : -1,\n",
    "          'objective': 'binary', \n",
    "          'nthread': 5, # Updated from nthread\n",
    "          'num_leaves': 64, \n",
    "          'learning_rate': 0.05, \n",
    "          'max_bin': 512, \n",
    "          'subsample_for_bin': 200,\n",
    "          'subsample': 1, \n",
    "          'subsample_freq': 1, \n",
    "          'colsample_bytree': 0.8, \n",
    "          'reg_alpha': 5, \n",
    "          'reg_lambda': 10,\n",
    "          'min_split_gain': 0.5, \n",
    "          'min_child_weight': 1, \n",
    "          'min_child_samples': 5, \n",
    "          'scale_pos_weight': 1,\n",
    "          'num_class' : 1,\n",
    "          'metric' : 'binary_error',\n",
    "          'cat_smooth' : 10,\n",
    "          'objective' : 'binary',\n",
    "          'metric' : 'auc'}\n",
    "\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = {\n",
    "    'learning_rate': [1, 0.5, 0.25, 0.1, 0.05, 0.01],\n",
    "    #'learning_rate': [0.05],\n",
    "    'n_estimators': [500],\n",
    "    \"max_depth\": [8],\n",
    "    'boosting_type' : ['gbdt'],\n",
    "    #'random_state' : [501], # Updated from 'seed'\n",
    "    'colsample_bytree' : [0.65],\n",
    "    'subsample' : [0.5],\n",
    "    'reg_alpha' : [1e-7, 1e-6, 1e-5, 1e-2, 0.1, 1, 100, 1000]\n",
    "    #'reg_lambda' : [1, 1.2, 1.4],\n",
    "    }\n",
    "\n",
    "# Number of random trials\n",
    "NUM_TRIALS = 1\n",
    "\n",
    "# We will use a Support Vector Classifier with \"rbf\" kernel\n",
    "#lgbm = LGBMClassifier()\n",
    "lgbm = LGBMClassifier(boosting_type= 'gbdt',  \n",
    "          n_jobs = -1, # Updated from 'nthread'\n",
    "          silent = True,\n",
    "          max_depth = params['max_depth'],\n",
    "          max_bin = params['max_bin'], \n",
    "          subsample_for_bin = params['subsample_for_bin'],\n",
    "          subsample = params['subsample'], \n",
    "          subsample_freq = params['subsample_freq'], \n",
    "          min_split_gain = params['min_split_gain'], \n",
    "          min_child_weight = params['min_child_weight'], \n",
    "          min_child_samples = params['min_child_samples'], \n",
    "          scale_pos_weight = params['scale_pos_weight'],\n",
    "          cat_smooth = params['cat_smooth'],\n",
    "          objective = params['objective'],\n",
    "          reg_alpha = params['reg_alpha'],\n",
    "          reg_lambda = params['reg_lambda'],\n",
    "          metric = params['metric'])\n",
    "\n",
    "start = time()\n",
    "(max_score, lgbm_best_estimator) = CrossValidationGridSearchNested(X_val, Y_val.ravel(), NUM_TRIALS, 10, lgbm, tuned_parameters, 'roc_auc')\n",
    "lgbm_best_parameter = lgbm_best_estimator.get_params()\n",
    "\n",
    "print(\"CrossValidationGridSearchNested of LightGradientBoostingClassifier wih NUM_TRIALS = %2.0d took %.2f seconds.\"%(NUM_TRIALS, (time() - start)))\n",
    "print(f'\\nmax_score = {max_score}\\n')\n",
    "print(f'\\nbest_estimator = {lgbm_best_estimator}\\n')\n",
    "print(f'\\nbest_parameter = {lgbm_best_parameter}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> progress = 100.0%\n",
      "CrossValidationGridSearchNested of LightGradientBoostingClassifier wih NUM_TRIALS =  1 took 144823.09 seconds.\n",
      "\n",
      "max_score = 0.9902967810165906\n",
      "\n",
      "\n",
      "best_estimator = LGBMClassifier(boosting_type='gbdt', cat_smooth=10, class_weight=None,\n",
      "        colsample_bytree=0.65, learning_rate=0.1, max_bin=512, max_depth=8,\n",
      "        metric='auc', min_child_samples=5, min_child_weight=1,\n",
      "        min_split_gain=0.5, n_estimators=500, n_jobs=-1, num_leaves=31,\n",
      "        objective='binary', random_state=None, reg_alpha=0.01,\n",
      "        reg_lambda=1, scale_pos_weight=1, silent=True, subsample=0.5,\n",
      "        subsample_for_bin=200, subsample_freq=1)\n",
      "\n",
      "\n",
      "best_parameter = {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 0.65, 'learning_rate': 0.1, 'max_depth': 8, 'min_child_samples': 5, 'min_child_weight': 1, 'min_split_gain': 0.5, 'n_estimators': 500, 'n_jobs': -1, 'num_leaves': 31, 'objective': 'binary', 'random_state': None, 'reg_alpha': 0.01, 'reg_lambda': 1, 'silent': True, 'subsample': 0.5, 'subsample_for_bin': 200, 'subsample_freq': 1, 'max_bin': 512, 'scale_pos_weight': 1, 'cat_smooth': 10, 'metric': 'auc'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb  \n",
    "from lightgbm import LGBMClassifier \n",
    "from time import time\n",
    "\n",
    "\n",
    "params = {'boosting_type': 'gbdt',\n",
    "          'max_depth' : -1,\n",
    "          'objective': 'binary', \n",
    "          'nthread': 5, # Updated from nthread\n",
    "          'num_leaves': 64, \n",
    "          'learning_rate': 0.05, \n",
    "          'max_bin': 512, \n",
    "          'subsample_for_bin': 200,\n",
    "          'subsample': 1, \n",
    "          'subsample_freq': 1, \n",
    "          'colsample_bytree': 0.8, \n",
    "          'reg_alpha': 5, \n",
    "          'reg_lambda': 10,\n",
    "          'min_split_gain': 0.5, \n",
    "          'min_child_weight': 1, \n",
    "          'min_child_samples': 5, \n",
    "          'scale_pos_weight': 1,\n",
    "          'num_class' : 1,\n",
    "          'metric' : 'binary_error',\n",
    "          'cat_smooth' : 10,\n",
    "          'objective' : 'binary',\n",
    "          'metric' : 'auc'}\n",
    "\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = {\n",
    "    'learning_rate': [0.1],\n",
    "    #'learning_rate': [0.05],\n",
    "    'n_estimators': [500],\n",
    "    \"max_depth\": [8],\n",
    "    'boosting_type' : ['gbdt'],\n",
    "    #'random_state' : [501], # Updated from 'seed'\n",
    "    'colsample_bytree' : [0.65],\n",
    "    'subsample' : [0.5],\n",
    "    'reg_alpha' : [1e-7, 1e-6, 1e-5, 1e-2, 0.1, 1, 100, 1000],\n",
    "    'reg_lambda' : [1e-7, 1e-6, 1e-5, 1e-2, 0.1, 1, 100, 1000],\n",
    "    }\n",
    "\n",
    "# Number of random trials\n",
    "NUM_TRIALS = 1\n",
    "\n",
    "# We will use a Support Vector Classifier with \"rbf\" kernel\n",
    "#lgbm = LGBMClassifier()\n",
    "lgbm = LGBMClassifier(boosting_type= 'gbdt',  \n",
    "          n_jobs = -1, # Updated from 'nthread'\n",
    "          silent = True,\n",
    "          max_depth = params['max_depth'],\n",
    "          max_bin = params['max_bin'], \n",
    "          subsample_for_bin = params['subsample_for_bin'],\n",
    "          subsample = params['subsample'], \n",
    "          subsample_freq = params['subsample_freq'], \n",
    "          min_split_gain = params['min_split_gain'], \n",
    "          min_child_weight = params['min_child_weight'], \n",
    "          min_child_samples = params['min_child_samples'], \n",
    "          scale_pos_weight = params['scale_pos_weight'],\n",
    "          cat_smooth = params['cat_smooth'],\n",
    "          objective = params['objective'],\n",
    "          reg_alpha = params['reg_alpha'],\n",
    "          reg_lambda = params['reg_lambda'],\n",
    "          metric = params['metric'])\n",
    "\n",
    "start = time()\n",
    "(max_score, lgbm_best_estimator) = CrossValidationGridSearchNested(X_val, Y_val.ravel(), NUM_TRIALS, 10, lgbm, tuned_parameters, 'roc_auc')\n",
    "lgbm_best_parameter = lgbm_best_estimator.get_params()\n",
    "\n",
    "print(\"CrossValidationGridSearchNested of LightGradientBoostingClassifier wih NUM_TRIALS = %2.0d took %.2f seconds.\"%(NUM_TRIALS, (time() - start)))\n",
    "print(f'\\nmax_score = {max_score}\\n')\n",
    "print(f'\\nbest_estimator = {lgbm_best_estimator}\\n')\n",
    "print(f'\\nbest_parameter = {lgbm_best_parameter}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
